{"cells":[{"cell_type":"markdown","source":["# Databricks Assessment -> Question 1 - Solution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d5f6ad72-6b62-43f3-8e11-9eb94d2d21a9"},{"cell_type":"code","source":["import pyspark.sql.functions as f\n","from pyspark.sql import SparkSession\n","\n","# create a spark session\n","schema = [\"sid\", \"id\", \"position\", \"created_at\", \"created_meta\", \"updated_at\", \"updated_meta\", \"meta\", \"year\", \"first_name\", \"county\", \"sex\", \"count\"]\n","baby_names_path = 'Files/data/baby_names.json'\n","\n","# Do not edit the code above this line.\n","########################################\n","\n","def read_json_and_flatten_data(spark, baby_names_path):\n","    \"\"\"\n","    Reads the JSON data from the provided path and pulls all columns in the nested data column to top level.\n","    \n","    Parameters:\n","    - spark: The SparkSession object.\n","    - baby_names_path: Path to the JSON file containing baby names data.\n","\n","    Returns:\n","    - A DataFrame.\n","    \"\"\"\n","    # Load JSON data\n","    df = spark.read.json(baby_names_path, multiLine=True)\n","    \n","    # Flatten the nested data\n","    df_flat = df.select(f.explode(\"data\").alias(\"data\")).selectExpr(\n","        \"data[0] as sid\",\n","        \"data[1] as id\",\n","        \"data[2] as position\",\n","        \"data[3] as created_at\",\n","        \"data[4] as created_meta\",\n","        \"data[5] as updated_at\",\n","        \"data[6] as updated_meta\",\n","        \"data[7] as meta\",\n","        \"data[8] as year\",\n","        \"data[9] as first_name\",\n","        \"data[10] as county\",\n","        \"data[11] as sex\",\n","        \"data[12] as count\"\n","    )\n","    return df_flat\n","\n","def parse_dataframe_with_schema(df_processed, schema):\n","    \"\"\"\n","    Parses the DataFrame returned by read_json_and_flatten_data for output to CSV based on the provided schema.\n","    \n","    Parameters:\n","    - df_processed: DataFrame returned from read_json_and_flatten_data.\n","    - schema: Schema to follow for the output CSV.\n","\n","    Returns:\n","    - A DataFrame processed based on the provided schema.\n","    \"\"\"\n","    # Select only the columns specified in the schema\n","    return df_processed.select(schema)\n","\n","########################################\n","# Do not edit the code below this line\n","df_processed = read_json_and_flatten_data(spark, baby_names_path)\n","df = parse_dataframe_with_schema(df_processed, schema)\n","display(df)\n","#df.toPandas().to_csv('data/databricks_baby_names.csv', index=False)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1fe51acb-f07a-4394-aec9-20459c89b47b"},{"cell_type":"markdown","source":["# Databricks Assessment -> Question 2 - Solution Pre-req Step"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee8b504e-715e-42ea-9187-3161f362ac68"},{"cell_type":"code","source":["df.createOrReplaceTempView(\"baby_names\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"968cbca0-929e-4816-8c49-3a17702d4454"},{"cell_type":"markdown","source":["# Databricks Assessment -> Question 2 - Solution (SQL)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99692409-dcff-4007-ba57-44ecbb54f2b0"},{"cell_type":"code","source":["%%sql\n","\n","SELECT year, first_name, count\n","FROM (\n","    SELECT\n","        year,\n","        first_name,\n","        count,\n","        ROW_NUMBER() OVER (PARTITION BY year ORDER BY CAST(count AS INT) DESC) as rn\n","    FROM baby_names\n",") ranked\n","WHERE rn = 1\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"98828be3-9d76-40ad-b012-1f37f0139331"},{"cell_type":"markdown","source":["# Databricks Assessment -> Question 2 - Solution (Python)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81fc2804-6615-44e6-aee8-d459818524de"},{"cell_type":"code","source":["from pyspark.sql import Window\n","import pyspark.sql.functions as F\n","\n","# Convert the 'count' column to integer (if it's not already)\n","df = df.withColumn(\"count\", F.col(\"count\").cast(\"int\"))\n","\n","# Define a window partitioned by year, ordered by count descending\n","window = Window.partitionBy(\"year\").orderBy(F.desc(\"count\"))\n","\n","# Add a row number to each row within the year partition\n","df_ranked = df.withColumn(\"rn\", F.row_number().over(window))\n","\n","# Filter to keep only the most popular name(s) for each year\n","df = df_ranked.filter(F.col(\"rn\") == 1).select(\"year\", \"first_name\")\n","\n","df.show()\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"fcebfec5-d4aa-40e1-a869-e347a5223cb1"},{"cell_type":"markdown","source":["# Databricks Assessment -> Question 3 - Solution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd89433f-1359-4633-9c66-f66c5c64dca7"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, explode, udf\n","from pyspark.sql.types import IntegerType, ArrayType, StructType, StringType, StructField\n","import xml.etree.ElementTree as ET\n","import re\n","\n","spark = SparkSession.builder.appName(\"databricks_de\").getOrCreate()\n","visitors_path = 'data/births-with-visitor-data.json'\n","\n","def _run_query(query):\n","    return spark.sql(query).collect()\n","\n","def _strip_margin(text):\n","    return re.sub('\\n[ \\t]*\\|', '\\n', text)\n","\n","# Load JSON data\n","df = spark.read.json(\"Files/data/births-with-visitor-data.json\")\n","\n","# UDF to parse XML and extract visitor data\n","def parse_visitors(xml_string):\n","    root = ET.fromstring(xml_string)\n","    visitors = []\n","    for visitor in root.findall('visitor'):\n","        visitors.append({\n","            'visitor_id': int(visitor.get('id')),\n","            'age': int(visitor.get('age')),\n","            'sex': visitor.get('sex')\n","        })\n","    return visitors\n","\n","# Register the UDF\n","parse_visitors_udf = udf(parse_visitors, ArrayType(StructType([\n","    StructField(\"visitor_id\", IntegerType()),\n","    StructField(\"age\", IntegerType()),\n","    StructField(\"sex\", StringType())\n","])))\n","\n","df = df.withColumn(\"visitor_data\", explode(parse_visitors_udf(col(\"visitors\"))))\n","df = df.select(\"sid\", \"created_at\", \"first_name\", \"county\", \"sex\", \"name_count\", \"visitor_data.*\")\n","\n","df.createOrReplaceTempView(\"tempVisitors\")\n","\n","### Part A\n","queryA = \"\"\"\n","    SELECT COUNT(sid) AS totalRecords\n","    FROM tempVisitors\n","\"\"\"\n","query_result = _run_query(queryA)\n","\n","try:\n","    partA = f\"records={query_result[0][0]}\"\n","except IndexError:\n","    partA = \"\"\n","\n","### Part B\n","queryB = \"\"\"\n","    SELECT county, AVG(visitor_count) AS avgVisitors\n","    FROM (\n","        SELECT sid, county, COUNT(visitor_id) AS visitor_count\n","        FROM tempVisitors\n","        GROUP BY sid, county\n","    ) AS visitor_counts\n","    GROUP BY county\n","    ORDER BY avgVisitors DESC\n","    LIMIT 1\n","\"\"\"\n","query_result = _run_query(queryB)\n","try:\n","    partB = f\"county={query_result[0][0]}, avgVisitors={query_result[0][1]}\"\n","except IndexError:\n","    partB = \"\"\n","\n","### Part C\n","queryC = \"\"\"\n","    SELECT AVG(age) AS avgVisitorAge\n","    FROM tempVisitors\n","    WHERE county = 'KINGS'\n","\"\"\"\n","query_result = _run_query(queryC)\n","try:\n","    partC = f\"avgVisitorAge={query_result[0][0]}\"\n","except IndexError:\n","    partC = \"\"\n","\n","### Part D\n","queryD = \"\"\"\n","    SELECT age AS mostCommonBirthAge, COUNT(*) AS count\n","    FROM tempVisitors\n","    WHERE county = 'KINGS'\n","    GROUP BY age\n","    ORDER BY count DESC\n","    LIMIT 1\n","\"\"\"\n","query_result = _run_query(queryD)\n","try:\n","    partD = f\"mostCommonBirthAge={query_result[0][0]}, count={query_result[0][1]}\"\n","except IndexError:\n","    partD = \"\"\n","\n","print(f\"{partA}\\n{partB}\\n{partC}\\n{partD}\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"2bd43ccb-e32a-406f-99d8-67e98e61f07e"}],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"sessionKeepAliveTimeout":0,"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"a365ComputeOptions":null,"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f72a73f7-9476-4440-9c6d-14aa8ea4e890","known_lakehouses":[{"id":"f72a73f7-9476-4440-9c6d-14aa8ea4e890"}],"default_lakehouse_name":"AdventureWorks","default_lakehouse_workspace_id":"55a8010c-5ccb-496e-8a86-f2f6a82e66c0"}}},"nbformat":4,"nbformat_minor":5}
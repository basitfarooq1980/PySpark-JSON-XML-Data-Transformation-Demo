{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as f\n","from pyspark.sql import SparkSession\n","\n","schema = [\"sid\", \"id\", \"position\", \"created_at\", \"created_meta\", \"updated_at\", \"updated_meta\", \"meta\", \"year\", \"first_name\", \"county\", \"sex\", \"count\"]\n","baby_names_path = 'Files/data/databricks_baby_names.json'\n","\n","\n","def read_json_and_flatten_data(spark, baby_names_path):\n","    \"\"\"\n","    Reads the JSON data from the provided path and pulls all columns in the nested data column to top level.\n","    \n","    Parameters:\n","    - spark: The SparkSession object.\n","    - baby_names_path: Path to the JSON file containing baby names data.\n","\n","    Returns:\n","    - A DataFrame with flattened data.\n","    \"\"\"\n","    # Read the JSON file\n","    df = spark.read.json(baby_names_path, multiLine=True)\n","    \n","    # Extract the 'data' field which contains the records\n","    df_data = df.select(f.explode(df.data).alias('record'))\n","    \n","    # Flatten the nested structure using getItem to access array elements\n","    for i, col_name in enumerate(schema):\n","        df_data = df_data.withColumn(col_name, f.col('record').getItem(i))\n","    \n","    # Drop the original 'record' column\n","    df_data = df_data.drop('record')\n","    \n","    return df_data\n","\n","def parse_dataframe_with_schema(df_processed, schema):\n","    \"\"\"\n","    Parses the DataFrame returned by read_json_and_flatten_data for output to CSV based on the provided schema.\n","    \n","    Parameters:\n","    - df_processed: DataFrame returned from read_json_and_flatten_data.\n","    - schema: Schema to follow for the output CSV.\n","\n","    Returns:\n","    - A DataFrame processed based on the provided schema.\n","    \"\"\"\n","    # Select columns based on the provided schema\n","    df_final = df_processed.select(*schema)\n","    \n","    return df_final\n","\n","\n","df_processed = read_json_and_flatten_data(spark, baby_names_path)\n","df = parse_dataframe_with_schema(df_processed, schema)\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"35171e46-8524-499b-8932-d9d6216821a0"},{"cell_type":"code","source":["import pyspark.sql.functions as f\n","from pyspark.sql import SparkSession\n","\n","# create a spark session\n","schema = [\"sid\", \"id\", \"position\", \"created_at\", \"created_meta\", \"updated_at\", \"updated_meta\", \"meta\", \"year\", \"first_name\", \"county\", \"sex\", \"count\"]\n","baby_names_path = 'Files/data/databricks_baby_names.json'\n","\n","# Do not edit the code above this line.\n","########################################\n","\n","def read_json_and_flatten_data(spark, baby_names_path):\n","    \"\"\"\n","    Reads the JSON data from the provided path and pulls all columns in the nested data column to top level.\n","    \n","    Parameters:\n","    - spark: The SparkSession object.\n","    - baby_names_path: Path to the JSON file containing baby names data.\n","\n","    Returns:\n","    - A DataFrame.\n","    \"\"\"\n","    # Load JSON data\n","    df = spark.read.json(baby_names_path, multiLine=True)\n","    \n","    # Flatten the nested data\n","    df_flat = df.select(f.explode(\"data\").alias(\"data\")).selectExpr(\n","        \"data[0] as sid\",\n","        \"data[1] as id\",\n","        \"data[2] as position\",\n","        \"data[3] as created_at\",\n","        \"data[4] as created_meta\",\n","        \"data[5] as updated_at\",\n","        \"data[6] as updated_meta\",\n","        \"data[7] as meta\",\n","        \"data[8] as year\",\n","        \"data[9] as first_name\",\n","        \"data[10] as county\",\n","        \"data[11] as sex\",\n","        \"data[12] as count\"\n","    )\n","    return df_flat\n","\n","def parse_dataframe_with_schema(df_processed, schema):\n","    \"\"\"\n","    Parses the DataFrame returned by read_json_and_flatten_data for output to CSV based on the provided schema.\n","    \n","    Parameters:\n","    - df_processed: DataFrame returned from read_json_and_flatten_data.\n","    - schema: Schema to follow for the output CSV.\n","\n","    Returns:\n","    - A DataFrame processed based on the provided schema.\n","    \"\"\"\n","    # Select only the columns specified in the schema\n","    return df_processed.select(schema)\n","\n","########################################\n","# Do not edit the code below this line\n","df_processed = read_json_and_flatten_data(spark, baby_names_path)\n","df = parse_dataframe_with_schema(df_processed, schema)\n","display(df)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1fe51acb-f07a-4394-aec9-20459c89b47b"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, expr, explode, regexp_extract, when\n","import re\n","\n","spark = SparkSession.builder.appName(\"databricks_de\").getOrCreate()\n","visitors_path = 'Files/data/births-with-visitor-data.json'\n","\n","def _run_query(query):\n","    return spark.sql(query).collect()\n","\n","def _strip_margin(text):\n","    return re.sub('\\n[ \\t]*\\|', '\\n', text)\n","\n","# Load the data\n","df = spark.read.json(visitors_path)\n","\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0260714b-19e0-4d7c-8f53-f388e49f4dc6"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, explode, udf\n","from pyspark.sql.types import IntegerType, ArrayType, StructType, StringType, StructField\n","import xml.etree.ElementTree as ET\n","import re\n","\n","spark = SparkSession.builder.appName(\"databricks_de\").getOrCreate()\n","visitors_path = 'data/births-with-visitor-data.json'\n","\n","def _run_query(query):\n","    return spark.sql(query).collect()\n","\n","def _strip_margin(text):\n","    return re.sub('\\n[ \\t]*\\|', '\\n', text)\n","\n","# Load JSON data\n","df = spark.read.json(\"Files/data/births-with-visitor-data.json\")\n","\n","# UDF to parse XML and extract visitor data\n","def parse_visitors(xml_string):\n","    root = ET.fromstring(xml_string)\n","    visitors = []\n","    for visitor in root.findall('visitor'):\n","        visitors.append({\n","            'visitor_id': int(visitor.get('id')),\n","            'age': int(visitor.get('age')),\n","            'sex': visitor.get('sex')\n","        })\n","    return visitors\n","\n","# Register the UDF\n","parse_visitors_udf = udf(parse_visitors, ArrayType(StructType([\n","    StructField(\"visitor_id\", IntegerType()),\n","    StructField(\"age\", IntegerType()),\n","    StructField(\"sex\", StringType())\n","])))\n","\n","df = df.withColumn(\"visitor_data\", explode(parse_visitors_udf(col(\"visitors\"))))\n","df = df.select(\"sid\", \"created_at\", \"first_name\", \"county\", \"sex\", \"name_count\", \"visitor_data.*\")\n","\n","df.createOrReplaceTempView(\"tempVisitors\")\n","\n","### Part A\n","queryA = \"\"\"\n","    SELECT COUNT(sid) AS totalRecords\n","    FROM tempVisitors\n","\"\"\"\n","query_result = _run_query(queryA)\n","\n","try:\n","    partA = f\"records={query_result[0][0]}\"\n","except IndexError:\n","    partA = \"\"\n","\n","### Part B\n","queryB = \"\"\"\n","    SELECT county, AVG(visitor_count) AS avgVisitors\n","    FROM (\n","        SELECT sid, county, COUNT(visitor_id) AS visitor_count\n","        FROM tempVisitors\n","        GROUP BY sid, county\n","    ) AS visitor_counts\n","    GROUP BY county\n","    ORDER BY avgVisitors DESC\n","    LIMIT 1\n","\"\"\"\n","query_result = _run_query(queryB)\n","try:\n","    partB = f\"county={query_result[0][0]}, avgVisitors={query_result[0][1]}\"\n","except IndexError:\n","    partB = \"\"\n","\n","### Part C\n","queryC = \"\"\"\n","    SELECT AVG(age) AS avgVisitorAge\n","    FROM tempVisitors\n","    WHERE county = 'KINGS'\n","\"\"\"\n","query_result = _run_query(queryC)\n","try:\n","    partC = f\"avgVisitorAge={query_result[0][0]}\"\n","except IndexError:\n","    partC = \"\"\n","\n","### Part D\n","queryD = \"\"\"\n","    SELECT age AS mostCommonBirthAge, COUNT(*) AS count\n","    FROM tempVisitors\n","    WHERE county = 'KINGS'\n","    GROUP BY age\n","    ORDER BY count DESC\n","    LIMIT 1\n","\"\"\"\n","query_result = _run_query(queryD)\n","try:\n","    partD = f\"mostCommonBirthAge={query_result[0][0]}, count={query_result[0][1]}\"\n","except IndexError:\n","    partD = \"\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"cc1b63c7-5054-4549-b7a4-010b17ca7db7"},{"cell_type":"code","source":["print(f\"{partA}\\n{partB}\\n{partC}\\n{partD}\\n\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e77dcd5-b277-481f-80b9-176ccc24f410"}],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"sessionKeepAliveTimeout":0,"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"a365ComputeOptions":null,"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"93124ba1-30b3-4f5d-a402-ff20eaf6e9b1","default_lakehouse_name":"ContosoLakehouse","default_lakehouse_workspace_id":"55a8010c-5ccb-496e-8a86-f2f6a82e66c0","known_lakehouses":[{"id":"93124ba1-30b3-4f5d-a402-ff20eaf6e9b1"}]}}},"nbformat":4,"nbformat_minor":5}